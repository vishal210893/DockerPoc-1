import org.apache.iceberg.catalog.Catalog;
import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.CatalogProperties;
import org.apache.iceberg.rest.RESTCatalog;

Map<String, String> properties = new HashMap<>();

properties.put(CatalogProperties.CATALOG_IMPL, "org.apache.iceberg.rest.RESTCatalog");
properties.put(CatalogProperties.URI, "http://rest:8181");
properties.put(CatalogProperties.WAREHOUSE_LOCATION, "s3a://warehouse/wh/");
properties.put(CatalogProperties.FILE_IO_IMPL, "org.apache.iceberg.aws.s3.S3FileIO");
properties.put(AwsProperties.S3FILEIO_ENDPOINT, "http://minio:9000");

RESTCatalog catalog = new RESTCatalog();
Configuration conf = new Configuration();
catalog.setConf(conf);
catalog.initialize("demo", properties);


import org.apache.iceberg.Table;
import org.apache.iceberg.TableScan;
import org.apache.iceberg.catalog.Namespace;
import org.apache.iceberg.catalog.TableIdentifier;

Namespace webapp = Namespace.of("webapp");
TableIdentifier name = TableIdentifier.of(webapp, "logs");
Table table = catalog.loadTable(name);

import org.apache.iceberg.io.CloseableIterable;
import org.apache.iceberg.data.Record;
import org.apache.iceberg.data.IcebergGenerics;

CloseableIterable<Record> result = IcebergGenerics.read(table).build();

for (Record r: result) {
    System.out.println(r);
}

import org.apache.iceberg.expressions.Expressions;

CloseableIterable<Record> result = IcebergGenerics.read(table)
        .where(Expressions.equal("level", "error"))
        .build();




spring:
  application:
    name: iceberg-s3-demo

aws:
  s3:
    access-key: YOUR_AWS_ACCESS_KEY
    secret-key: YOUR_AWS_SECRET_KEY
    region: us-east-1   # e.g. us-east-1, eu-west-1, etc.

iceberg:
  catalog-name: demo
  catalog-uri: http://rest:8181
  warehouse-location: s3a://my-iceberg-bucket/warehouse


package com.example.icebergs3demo.config;

import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.CatalogProperties;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.rest.RESTCatalog;
import org.apache.iceberg.aws.AwsProperties;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class IcebergConfig {

  @Value("${iceberg.catalog-name}")
  private String catalogName;

  @Value("${iceberg.catalog-uri}")
  private String catalogUri;

  @Value("${iceberg.warehouse-location}")
  private String warehouseLocation;

  @Value("${aws.s3.access-key}")
  private String accessKey;

  @Value("${aws.s3.secret-key}")
  private String secretKey;

  @Value("${aws.s3.region}")
  private String region;

  @Bean
  public RESTCatalog icebergCatalog() {
    // 1) Hadoop config for S3
    Configuration hadoopConf = new Configuration();
    hadoopConf.set("fs.s3a.access.key", accessKey);
    hadoopConf.set("fs.s3a.secret.key", secretKey);
    hadoopConf.set("fs.s3a.endpoint", "s3.amazonaws.com");
    hadoopConf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem");
    hadoopConf.set("fs.s3a.path.style.access", "true");
    hadoopConf.set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider");
    hadoopConf.set("fs.s3a.connection.maximum", "100");
    hadoopConf.set("fs.s3a.region", region);

    // 2) Iceberg catalog properties
    Map<String, String> props = new HashMap<>();
    props.put(CatalogProperties.CATALOG_IMPL, RESTCatalog.class.getName());
    props.put(CatalogProperties.URI, catalogUri);
    props.put(CatalogProperties.WAREHOUSE_LOCATION, warehouseLocation);
    props.put(CatalogProperties.FILE_IO_IMPL, "org.apache.iceberg.aws.s3.S3FileIO");
    props.put(AwsProperties.S3FILEIO_ACCESS_KEY_ID, accessKey);
    props.put(AwsProperties.S3FILEIO_SECRET_ACCESS_KEY, secretKey);
    props.put(AwsProperties.S3FILEIO_REGION, region);

    // 3) Initialize
    RESTCatalog catalog = new RESTCatalog();
    catalog.setConf(hadoopConf);
    catalog.initialize(catalogName, props);
    return catalog;
  }
}


package com.example.icebergs3demo.service;

import javax.annotation.PostConstruct;

import org.apache.iceberg.Table;
import org.apache.iceberg.TableScan;
import org.apache.iceberg.catalog.Namespace;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.data.CloseableIterable;
import org.apache.iceberg.data.Record;
import org.apache.iceberg.data.IcebergGenerics;
import org.apache.iceberg.expressions.Expressions;
import org.apache.iceberg.rest.RESTCatalog;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

@Service
public class IcebergScanService {
  private static final Logger log = LoggerFactory.getLogger(IcebergScanService.class);
  private final RESTCatalog catalog;

  public IcebergScanService(RESTCatalog catalog) {
    this.catalog = catalog;
  }

  @PostConstruct
  public void runScan() {
    // 1) Load the table
    TableIdentifier name = TableIdentifier.of(Namespace.of("webapp"), "logs");
    Table table = catalog.loadTable(name);

    // 2) Full scan via IcebergGenerics
    log.info("=== Full table scan ===");
    try (CloseableIterable<Record> all = IcebergGenerics.read(table).build()) {
      for (Record rec : all) {
        log.info(rec.toString());
      }
    }

    // 3) Filtered scan: only error-level logs
    log.info("=== Filtered scan (level = 'error') ===");
    try (CloseableIterable<Record> errors =
             IcebergGenerics.read(table)
                             .where(Expressions.equal("level", "error"))
                             .build()) {
      for (Record rec : errors) {
        log.info(rec.toString());
      }
    }

    // 4) Plan scan tasks (e.g. for a query engine)
    log.info("=== Planning scan tasks for info-level only ===");
    TableScan scan = table.newScan()
                          .filter(Expressions.equal("level", "info"))
                          .select("message");
    scan.planTasks().forEach(task ->
      task.files().forEach(fileTask ->
        log.info("DataFile: {}", fileTask.file())
      )
    );
  }
}


<project>
  <!-- … your existing project coordinates … -->

  <properties>
    <!-- align these versions with your Spark / Iceberg distro -->
    <iceberg.version>1.4.0</iceberg.version>
    <hadoop.version>3.3.1</hadoop.version>
    <java.version>11</java.version>
  </properties>

  <dependencies>
    <!-- Spring Boot core -->
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
    </dependency>

    <!-- Apache Iceberg core APIs -->
    <dependency>
      <groupId>org.apache.iceberg</groupId>
      <artifactId>iceberg-core</artifactId>
      <version>${iceberg.version}</version>
    </dependency>

    <!-- REST catalog implementation -->
    <dependency>
      <groupId>org.apache.iceberg</groupId>
      <artifactId>iceberg-rest</artifactId>
      <version>${iceberg.version}</version>
    </dependency>

    <!-- S3 File I/O for Iceberg -->
    <dependency>
      <groupId>org.apache.iceberg</groupId>
      <artifactId>iceberg-s3</artifactId>
      <version>${iceberg.version}</version>
    </dependency>

    <!-- Hadoop AWS for fs.s3a:// support -->
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-aws</artifactId>
      <version>${hadoop.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>${hadoop.version}</version>
    </dependency>

    <!-- AWS SDK (v1 bundle, pulled in by hadoop-aws but you can override) -->
    <dependency>
      <groupId>com.amazonaws</groupId>
      <artifactId>aws-java-sdk-bundle</artifactId>
      <version>1.12.300</version>
    </dependency>
  </dependencies>

  <build>
    <plugins>
      <!-- Spring Boot Maven plugin -->
      <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
      </plugin>
    </plugins>
  </build>
</project>
